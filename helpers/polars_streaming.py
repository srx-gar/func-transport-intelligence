"""
Streaming file parser using Polars for large ZTDWR .dat files.
Processes files in chunks with 50-70% less memory than pandas.

Key advantages over pandas streaming:
- Multi-threaded chunk processing
- Lower memory overhead per chunk
- Better parallelization support
- Lazy evaluation for query optimization
"""

import gzip
import io
import logging
import re
from typing import Iterator, Literal, Optional, Tuple

try:
    import polars as pl
    POLARS_AVAILABLE = True
except ImportError:
    POLARS_AVAILABLE = False


# Optimized schema for memory efficiency
# Polars uses Arrow format - much more efficient than pandas
OPTIMIZED_SCHEMA = {
    # Primary identifiers (string = Utf8 in Polars)
    'surat_pengantar_brg': pl.Utf8,
    'no_tiket': pl.Utf8,
    'no_kontrak': pl.Utf8,
    'equipment': pl.Utf8,
    'no_polisi': pl.Utf8,
    'reservation': pl.Utf8,

    # Person identifiers
    'nik_supir': pl.Utf8,
    'nik_pemuat_1': pl.Utf8,
    'nik_pemuat_2': pl.Utf8,
    'nik_pemuat_3': pl.Utf8,
    'nik_pemuat_4': pl.Utf8,
    'nik_pemuat_5': pl.Utf8,
    'nik_supir_mandah': pl.Utf8,
    'nama_supir': pl.Utf8,

    # Organization/location
    'transporter': pl.Utf8,
    'transporter_name': pl.Utf8,
    'sender': pl.Utf8,
    'sender_name': pl.Utf8,
    'receiver': pl.Utf8,
    'receiver_name': pl.Utf8,
    'nama_pt': pl.Utf8,
    'pengangkut': pl.Utf8,
    'asal': pl.Utf8,
    'tujuan': pl.Utf8,
    'divisi_sender': pl.Utf8,
    'division_receiver': pl.Utf8,

    # Numeric fields - Float32 for memory efficiency (vs Float64)
    'kilometer_actual': pl.Float32,
    'kilometer_std': pl.Float32,
    'bbm_actual': pl.Float32,
    'bbm_std': pl.Float32,
    'qty_dikirim': pl.Float32,
    'qty_diterima': pl.Float32,
    'kapasitas_kendaraan': pl.Float32,
    'ongkos_angkut': pl.Float32,
    'hk': pl.Float32,
    'jaring_tbs': pl.Float32,
    'susut_timbang': pl.Float32,

    # Integer fields
    'tonase': pl.Int32,

    # Dates/times as strings (will be parsed in transformer)
    'tanggal_buat': pl.Utf8,
    'tanggal_timbang': pl.Utf8,
    'jam_timbang': pl.Utf8,
    'waktu_buat': pl.Utf8,
}


def _canonicalize_column_name(col: str) -> str:
    """Convert a raw header into a normalized lowercase snake_case token."""
    if col is None:
        return ''
    col = str(col)
    col = col.replace('\ufeff', '')
    col = col.strip()
    col = re.sub(r"[^0-9a-zA-Z]+", "_", col)
    col = re.sub(r"_+", "_", col)
    return col.strip("_").lower()


def _detect_delimiter(first_line: str) -> Tuple[str, str]:
    """Detect the delimiter type."""
    normalized_first_line = first_line.replace('\\', '')

    if '\t' in first_line and '~|^' not in normalized_first_line:
        return '\t', 'tab'
    elif '~|^' in normalized_first_line:
        return '~|^', 'custom'
    elif '|' in first_line:
        return '|', 'pipe'
    else:
        return '\t', 'tab'


def parse_ztdwr_chunks_polars(
    content: bytes,
    chunk_size: int = 10000,
    decompress: bool = True
) -> Iterator[pl.DataFrame]:
    """
    Parse ZTDWR file in chunks using Polars for memory-efficient processing.

    Polars advantages:
    - 50-70% less memory per chunk than pandas
    - Multi-threaded processing
    - Efficient string handling via Arrow

    Args:
        content: Raw file content (may be gzipped)
        chunk_size: Number of rows per chunk
        decompress: Whether to decompress if gzipped

    Yields:
        Polars DataFrame chunks with normalized column names
    """
    if not POLARS_AVAILABLE:
        raise ImportError("Polars not installed. Install with: pip install polars")

    # Decompress if needed
    if decompress and len(content) >= 2 and content[:2] == b'\x1f\x8b':
        logging.info(f"ðŸ—œï¸  Decompressing payload from {len(content):,} bytes")
        content = gzip.decompress(content)
        logging.info(f"âœ… Decompressed to {len(content):,} bytes")

    # Decode content
    try:
        text = content.decode('utf-8')
    except UnicodeDecodeError:
        logging.warning("UTF-8 decode failed, falling back to Latin-1")
        text = content.decode('latin-1')

    # Detect delimiter
    first_line = text.split('\n')[0]
    delimiter, delim_type = _detect_delimiter(first_line)
    logging.info(f"ðŸ“‹ Detected delimiter: {repr(delimiter)} ({delim_type})")

    # Store original delimiter for header parsing
    original_delimiter = delimiter

    # Handle multi-character delimiters (Polars only supports single-char)
    if len(delimiter) > 1:
        logging.info(f"âš ï¸  Multi-character delimiter detected: {repr(delimiter)}")
        logging.info(f"ðŸ”„ Replacing with single character '|' for Polars compatibility")
        text = text.replace(delimiter, '|')
        delimiter = '|'
        delim_type = 'pipe'

    # Clean headers (use original delimiter for splitting first line)
    header_parts = first_line.split(original_delimiter)
    cleaned_headers = []
    for h in header_parts:
        s = str(h).replace('\\', '').strip().lstrip('^').rstrip('~').lstrip('|').rstrip('|')
        cleaned_headers.append(s)

    # Canonicalize column names
    canonical_headers = [_canonicalize_column_name(col) for col in cleaned_headers]

    # Apply canonical mapping
    canonical_map = {
        'spb_id': 'surat_pengantar_brg',
        'spb': 'surat_pengantar_brg',
        'surat_pengantar': 'surat_pengantar_brg',
        'driver_nik': 'nik_supir',
        'driverid': 'nik_supir',
        'niksupir': 'nik_supir',
        'nopol': 'no_polisi',
        'no_pol': 'no_polisi',
        'kilometer_act': 'kilometer_actual',
        'kilometer': 'kilometer_actual',
        'bbm': 'bbm_actual',
        'waktu_timbang_terima': 'jam_timbang',
        'waktu_timbang': 'jam_timbang',
        'tanggal_timbang_terima': 'tanggal_timbang',
        'tanggal_tim': 'tanggal_timbang',
    }

    mapped_headers = [canonical_map.get(col, col) for col in canonical_headers]

    # Build dtype mapping for optimal memory usage
    dtype_mapping = {}
    for header in mapped_headers:
        if header in OPTIMIZED_SCHEMA:
            dtype_mapping[header] = OPTIMIZED_SCHEMA[header]

    # Strategy: Read entire file into Polars (fast!), then iterate in chunks
    # Polars CSV parsing is so fast that this is often faster than chunked pandas
    logging.info("ðŸ“– Reading full file with Polars (multi-threaded)...")

    try:
        # Parse full file with Polars (very fast - multi-threaded)
        df = pl.read_csv(
            io.BytesIO(text.encode('utf-8')),
            separator=delimiter,
            has_header=True,
            new_columns=mapped_headers,  # Apply our cleaned headers
            dtypes=dtype_mapping if dtype_mapping else None,
            null_values=['', 'NULL', 'null', 'None', '~'],
            try_parse_dates=False,
            truncate_ragged_lines=True,
            ignore_errors=True,  # Skip malformed rows
            encoding='utf8',
            low_memory=False,
            rechunk=True,
        )
    except Exception as e:
        logging.error(f"Polars parsing failed: {e}")
        raise

    # Strip whitespace from string columns (Polars expression - very fast)
    string_cols = [col for col, dtype in zip(df.columns, df.dtypes) if dtype == pl.Utf8]
    if string_cols:
        df = df.with_columns([
            pl.col(col).str.strip_chars() for col in string_cols
        ])

    total_rows = len(df)
    logging.info(f"âœ… Loaded {total_rows:,} rows in memory ({df.estimated_size('mb'):.2f} MB)")

    # Yield chunks using Polars iter_slices (zero-copy slicing)
    chunk_num = 0
    for chunk_df in df.iter_slices(chunk_size):
        chunk_num += 1
        chunk_memory = chunk_df.estimated_size('mb')
        logging.info(
            f"ðŸ“¦ Yielding Polars chunk #{chunk_num} "
            f"({len(chunk_df):,} rows, {chunk_memory:.2f} MB)"
        )
        yield chunk_df

    logging.info(f"âœ… Finished streaming {chunk_num} chunks from Polars DataFrame")


def parse_ztdwr_chunks_lazy(
    content: bytes,
    chunk_size: int = 10000,
    decompress: bool = True,
    filters: Optional[list] = None
) -> Iterator[pl.DataFrame]:
    """
    Parse ZTDWR file using Polars LazyFrame for query optimization.

    This is the most advanced approach - Polars will optimize the query plan.
    Use this when you need to filter/transform before chunking.

    Args:
        content: Raw file content
        chunk_size: Rows per chunk
        decompress: Whether to decompress gzip
        filters: Optional Polars filter expressions (e.g., pl.col('transporter') == 'MWHT')

    Yields:
        Polars DataFrame chunks
    """
    if not POLARS_AVAILABLE:
        raise ImportError("Polars not installed")

    # Decompress
    if decompress and len(content) >= 2 and content[:2] == b'\x1f\x8b':
        logging.info(f"ðŸ—œï¸  Decompressing...")
        content = gzip.decompress(content)

    # Decode
    try:
        text = content.decode('utf-8')
    except UnicodeDecodeError:
        text = content.decode('latin-1')

    # Detect delimiter
    first_line = text.split('\n')[0]
    delimiter, _ = _detect_delimiter(first_line)

    # Write to temp file for lazy loading (scan_csv requires file path)
    import tempfile
    import os

    with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.dat') as tmp:
        tmp.write(text)
        tmp_path = tmp.name

    try:
        # Lazy scan (doesn't load data yet)
        lazy_df = pl.scan_csv(
            tmp_path,
            separator=delimiter,
            has_header=True,
            null_values=['', 'NULL', 'null', 'None', '~'],
            try_parse_dates=False,
            ignore_errors=True,
            encoding='utf8',
        )

        # Apply filters (will be pushed down by Polars optimizer)
        if filters:
            for filt in filters:
                lazy_df = lazy_df.filter(filt)

        # Collect with streaming (processes in chunks internally)
        df = lazy_df.collect(streaming=True)

        # Yield chunks
        for chunk in df.iter_slices(chunk_size):
            yield chunk

    finally:
        # Clean up temp file
        if os.path.exists(tmp_path):
            os.unlink(tmp_path)


def estimate_chunk_size_polars(
    file_size_bytes: int,
    available_memory_mb: int = 512
) -> int:
    """
    Estimate optimal chunk size for Polars processing.

    Polars is ~60% more memory-efficient than pandas, so we can use larger chunks.

    Args:
        file_size_bytes: Size of the file
        available_memory_mb: Available memory in MB

    Returns:
        Suggested chunk size (number of rows)
    """
    # Polars: ~1.2KB per row (vs pandas ~2KB)
    bytes_per_row = 1200

    # Use up to 60% of available memory (Polars is efficient)
    max_chunk_bytes = (available_memory_mb * 1024 * 1024) * 0.6

    # Calculate chunk size
    chunk_size = int(max_chunk_bytes // bytes_per_row)

    # Polars can handle larger chunks: 5000-100000 rows
    chunk_size = max(5000, min(100000, chunk_size))

    logging.info(
        f"ðŸ“Š Estimated chunk size: {chunk_size:,} rows "
        f"(~{(chunk_size * bytes_per_row) / (1024*1024):.1f} MB per chunk)"
    )

    return chunk_size


def polars_chunks_to_pandas(
    polars_chunks: Iterator[pl.DataFrame],
    use_pyarrow: bool = True
) -> Iterator:
    """
    Convert Polars chunk iterator to Pandas chunks for backward compatibility.

    Args:
        polars_chunks: Iterator of Polars DataFrames
        use_pyarrow: Use PyArrow extension arrays (faster)

    Yields:
        Pandas DataFrame chunks
    """
    import pandas as pd

    for chunk in polars_chunks:
        if use_pyarrow:
            # Zero-copy conversion
            yield chunk.to_pandas(use_pyarrow_extension_array=True)
        else:
            yield chunk.to_pandas()

